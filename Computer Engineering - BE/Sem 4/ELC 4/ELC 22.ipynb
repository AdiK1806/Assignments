{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELC Project NLP '22\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitted by:\n",
    "* Kunal Demla 102003088\n",
    "* Gaurav Pahwa 102003087"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract word vectors\n",
    "word_embeddings = {}\n",
    "file = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in file:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_embeddings[word] = coefs\n",
    "file.close()\n",
    "len(word_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit the `filename` to the path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the file\n",
    "filename=\"C:\\\\Users\\\\Gaurav Pahwa\\\\Downloads\\\\datasets\\\\news_articles\\\\002.txt\"\n",
    "f = open((filename), \"r\")#creating a file object\n",
    "df=f.read() #Read the contents of the file into text\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dollar gains on Greenspan speech\\n\\nThe dollar has hit its highest level against the euro in almost three months after the Federal Reserve head said the US trade deficit is set to stabilise.\\n\\nAnd Alan Greenspan highlighted the US government\\'s willingness to curb spending and rising household savings as factors which may help to reduce it. In late trading in New York, the dollar reached $1.2871 against the euro, from $1.2974 on Thursday. Market concerns about the deficit has hit the greenback in recent months. On Friday, Federal Reserve chairman Mr Greenspan\\'s speech in London ahead of the meeting of G7 finance ministers sent the dollar higher after it had earlier tumbled on the back of worse-than-expected US jobs data. \"I think the chairman\\'s taking a much more sanguine view on the current account deficit than he\\'s taken for some time,\" said Robert Sinche, head of currency strategy at Bank of America in New York. \"He\\'s taking a longer-term view, laying out a set of conditions under which the current account deficit can improve this year and next.\"\\n\\nWorries about the deficit concerns about China do, however, remain. China\\'s currency remains pegged to the dollar and the US currency\\'s sharp falls in recent months have therefore made Chinese export prices highly competitive. But calls for a shift in Beijing\\'s policy have fallen on deaf ears, despite recent comments in a major Chinese newspaper that the \"time is ripe\" for a loosening of the peg. The G7 meeting is thought unlikely to produce any meaningful movement in Chinese policy. In the meantime, the US Federal Reserve\\'s decision on 2 February to boost interest rates by a quarter of a point - the sixth such move in as many months - has opened up a differential with European rates. The half-point window, some believe, could be enough to keep US assets looking more attractive, and could help prop up the dollar. The recent falls have partly been the result of big budget deficits, as well as the US\\'s yawning current account gap, both of which need to be funded by the buying of US bonds and assets by foreign firms and governments. The White House will announce its budget on Monday, and many commentators believe the deficit will remain at close to half a trillion dollars.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dollar gains on Greenspan speech\n",
      "\n",
      "The dollar has hit its highest level against the euro in almost three months after the Federal Reserve head said the US trade deficit is set to stabilise.\n",
      "\n",
      "And Alan Greenspan highlighted the US government's willingness to curb spending and rising household savings as factors which may help to reduce it. In late trading in New York, the dollar reached $1.2871 against the euro, from $1.2974 on Thursday. Market concerns about the deficit has hit the greenback in recent months. On Friday, Federal Reserve chairman Mr Greenspan's speech in London ahead of the meeting of G7 finance ministers sent the dollar higher after it had earlier tumbled on the back of worse-than-expected US jobs data. \"I think the chairman's taking a much more sanguine view on the current account deficit than he's taken for some time,\" said Robert Sinche, head of currency strategy at Bank of America in New York. \"He's taking a longer-term view, laying out a set of conditions under which the current account deficit can improve this year and next.\"\n",
      "\n",
      "Worries about the deficit concerns about China do, however, remain. China's currency remains pegged to the dollar and the US currency's sharp falls in recent months have therefore made Chinese export prices highly competitive. But calls for a shift in Beijing's policy have fallen on deaf ears, despite recent comments in a major Chinese newspaper that the \"time is ripe\" for a loosening of the peg. The G7 meeting is thought unlikely to produce any meaningful movement in Chinese policy. In the meantime, the US Federal Reserve's decision on 2 February to boost interest rates by a quarter of a point - the sixth such move in as many months - has opened up a differential with European rates. The half-point window, some believe, could be enough to keep US assets looking more attractive, and could help prop up the dollar. The recent falls have partly been the result of big budget deficits, as well as the US's yawning current account gap, both of which need to be funded by the buying of US bonds and assets by foreign firms and governments. The White House will announce its budget on Monday, and many commentators believe the deficit will remain at close to half a trillion dollars.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Converting the DataFrame into a dictionary\n",
    "text_dictionary = {}\n",
    "text_dictionary[1] = df   \n",
    "# print(text_dictionary[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove stopwords\n",
    "def remove_stopwords(sen):\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
    "    return sen_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to make vectors out of the sentences\n",
    "def sentence_vector_func (sentences_cleaned) : \n",
    "    sentence_vector = []\n",
    "    for i in sentences_cleaned:\n",
    "        if len(i) != 0:\n",
    "            v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "        else:\n",
    "            v = np.zeros((100,))\n",
    "        sentence_vector.append(v)\n",
    "    \n",
    "    return (sentence_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the summary of the articles\n",
    "# NOTE - Remove '#' infront of print statement for displaying the contents at different stages of the text summarisation process\n",
    "def summary_text (test_text, n = 5):\n",
    "    sentences = []\n",
    "    \n",
    "    # tokenising the text \n",
    "    sentences.append(sent_tokenize(test_text))\n",
    "    # print(sentences)\n",
    "    sentences = [y for x in sentences for y in x] # flatten list\n",
    "    # print(sentences)\n",
    "    \n",
    "    # remove punctuations, numbers and special characters\n",
    "    clean_sentences = pd.Series(sentences).str.replace(\"[^a-z A-Z 0-9]\", \" \")\n",
    "\n",
    "    # make alphabets lowercase\n",
    "    clean_sentences = [s.lower() for s in clean_sentences]\n",
    "    #print(clean_sentences)\n",
    "\n",
    "    \n",
    "    # remove stopwords from the sentences\n",
    "    clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
    "    #print(clean_sentences)\n",
    "    \n",
    "    sentence_vectors = sentence_vector_func(clean_sentences)\n",
    "    \n",
    "    # similarity matrix\n",
    "    sim_mat = np.zeros([len(sentences), len(sentences)])\n",
    "    #print(sim_mat)\n",
    "    \n",
    "    # Finding the similarities between the sentences \n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(len(sentences)):\n",
    "            if i != j:\n",
    "                sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n",
    "    \n",
    "    \n",
    "    nx_graph = nx.from_numpy_array(sim_mat)\n",
    "    scores = nx.pagerank(nx_graph)\n",
    "    #print(scores)\n",
    "    \n",
    "    ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)))\n",
    "    # Extract sentences as the summary\n",
    "    summarised_string = ''\n",
    "    for i in range(n):\n",
    "        \n",
    "        try:\n",
    "            summarised_string = summarised_string + str(ranked_sentences[i][1])            \n",
    "        except IndexError:\n",
    "            print (\"Summary Not Available\")\n",
    "    \n",
    "    return (summarised_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kindly let me know in how many sentences you want the summary - \n",
      "8\n",
      "Summary of the article -  1\n",
      "The G7 meeting is thought unlikely to produce any meaningful movement in Chinese policy.In late trading in New York, the dollar reached $1.2871 against the euro, from $1.2974 on Thursday.Worries about the deficit concerns about China do, however, remain.Market concerns about the deficit has hit the greenback in recent months.But calls for a shift in Beijing's policy have fallen on deaf ears, despite recent comments in a major Chinese newspaper that the \"time is ripe\" for a loosening of the peg.The half-point window, some believe, could be enough to keep US assets looking more attractive, and could help prop up the dollar.And Alan Greenspan highlighted the US government's willingness to curb spending and rising household savings as factors which may help to reduce it.\"I think the chairman's taking a much more sanguine view on the current account deficit than he's taken for some time,\" said Robert Sinche, head of currency strategy at Bank of America in New York.\n",
      "========================================================================================================================\n",
      "**************************************** The process has been completed successfully ****************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GAURAV~1\\AppData\\Local\\Temp/ipykernel_15584/4148469376.py:13: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  clean_sentences = pd.Series(sentences).str.replace(\"[^a-z A-Z 0-9]\", \" \")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Kindly let me know in how many sentences you want the summary - \")\n",
    "x = int(input())\n",
    "\n",
    "summary_dictionary = {}\n",
    "\n",
    "for key in text_dictionary:\n",
    "    \n",
    "    para = text_dictionary[key]\n",
    "    print(\"Summary of the article - \",key)\n",
    "    summary = summary_text(para,x)\n",
    "    summary_dictionary[key] = summary\n",
    "    \n",
    "    print(summary)\n",
    "    print('='*120)    \n",
    "    \n",
    "print (\"*\"*40,\"The process has been completed successfully\",\"*\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please enter file path in `load_files` at line 13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kunal and Gaurav winner.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import sys\n",
    "import os\n",
    "import string\n",
    "import math\n",
    "\n",
    "FILE_MATCHES = 1\n",
    "SENTENCE_MATCHES = 1\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Calculate IDF values across files\n",
    "    files = load_files('D:\\\\College HW\\\\ELC 4\\\\New folder')\n",
    "    file_words = {\n",
    "        filename: tokenize(files[filename])\n",
    "        for filename in files\n",
    "    }\n",
    "    file_idfs = compute_idfs(file_words)\n",
    "\n",
    "    # Prompt for query\n",
    "    query = set(tokenize(input(\"Query: \")))\n",
    "\n",
    "    # Determine top file matches according to TF-IDF\n",
    "    filenames = top_files(query, file_words, file_idfs, n=FILE_MATCHES)\n",
    "\n",
    "    # Extract sentences from top files\n",
    "    sentences = dict()\n",
    "    for filename in filenames:\n",
    "        for passage in files[filename].split(\"\\n\"):\n",
    "            for sentence in nltk.sent_tokenize(passage):\n",
    "                tokens = tokenize(sentence)\n",
    "                if tokens:\n",
    "                    sentences[sentence] = tokens\n",
    "\n",
    "    # Compute IDF values across sentences\n",
    "    idfs = compute_idfs(sentences)\n",
    "\n",
    "    # Determine top sentence matches\n",
    "    matches = top_sentences(query, sentences, idfs, n=SENTENCE_MATCHES)\n",
    "    for match in matches:\n",
    "        print(match)\n",
    "\n",
    "\n",
    "def load_files(directory):\n",
    "  \n",
    "    dictionary = {}\n",
    "\n",
    "    for file in os.listdir(directory):\n",
    "        with open(os.path.join(directory, file), encoding=\"utf-8\") as ofile:\n",
    "            dictionary[file] = ofile.read()\n",
    "\n",
    "    return dictionary\n",
    "\n",
    "\n",
    "def tokenize(document):\n",
    "   \n",
    "\n",
    "    tokenized = nltk.tokenize.word_tokenize(document.lower())\n",
    "\n",
    "    final_list = [x for x in tokenized if x not in string.punctuation and x not in nltk.corpus.stopwords.words(\"english\")]\n",
    "\n",
    "    return final_list\n",
    "\n",
    "\n",
    "def compute_idfs(documents):\n",
    "   \n",
    "    idf_dictio = {}\n",
    "    doc_len = len(documents)\n",
    "\n",
    "    unique_words = set(sum(documents.values(), []))\n",
    "\n",
    "    for word in unique_words:\n",
    "        count = 0\n",
    "        for doc in documents.values():\n",
    "            if word in doc:\n",
    "                count += 1\n",
    "\n",
    "        idf_dictio[word] = math.log(doc_len/count)\n",
    "\n",
    "    return idf_dictio\n",
    "\n",
    "\n",
    "def top_files(query, files, idfs, n):\n",
    "   \n",
    "    scores = {}\n",
    "    for filename, filecontent in files.items():\n",
    "        file_score = 0\n",
    "        for word in query:\n",
    "            if word in filecontent:\n",
    "                file_score += filecontent.count(word) * idfs[word]\n",
    "        if file_score != 0:\n",
    "            scores[filename] = file_score\n",
    "\n",
    "    sorted_by_score = [k for k, v in sorted(scores.items(), key=lambda x: x[1], reverse=True)]\n",
    "    return sorted_by_score[:n]\n",
    "\n",
    "\n",
    "def top_sentences(query, sentences, idfs, n):\n",
    "   \n",
    "    scores = {}\n",
    "    for sentence, sentwords in sentences.items():\n",
    "        score = 0\n",
    "        for word in query:\n",
    "            if word in sentwords:\n",
    "                score += idfs[word]\n",
    "\n",
    "        if score != 0:\n",
    "            density = sum([sentwords.count(x) for x in query]) / len(sentwords)\n",
    "            scores[sentence] = (score, density)\n",
    "\n",
    "    sorted_by_score = [k for k, v in sorted(scores.items(), key=lambda x: (x[1][0], x[1][1]), reverse=True)]\n",
    "\n",
    "    return sorted_by_score[:n]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5addf786bcd861d1ce5006f23111f8cbb206731e5b61b0a5632ba9e0252558a8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
